üìö Chat with Your PDFs (RAG Application)

Project Overview

This project implements a powerful "Chat with Your PDFs" application that allows users to ask questions about their uploaded PDF documents and get answers generated by an AI. It leverages Retrieval-Augmented Generation (RAG), combining document retrieval with a large language model to provide accurate and context-aware responses.

The application is built with Streamlit for an interactive web interface, LangChain for orchestrating the RAG pipeline, FAISS as the vector store for efficient document search, and Google's Gemini API for text embeddings and intelligent conversational responses.

‚ú® Features

PDF Document Upload: Easily upload one or multiple PDF files directly through the web interface.

Intelligent Text Extraction: Extracts text content from uploaded PDFs, even from complex layouts.

Text Chunking: Breaks down large documents into smaller, manageable, overlapping chunks to optimize retrieval and context feeding to the LLM.

Vector Embeddings: Converts text chunks into high-dimensional numerical embeddings using Google's powerful embedding models, capturing the semantic meaning of the text.

FAISS Vector Store: Stores these embeddings in a FAISS (Facebook AI Similarity Search) index, enabling rapid and efficient semantic similarity searches for relevant document parts.

Conversational AI (RAG):

When a user asks a question, the system retrieves the most relevant text chunks from the vector store.

These retrieved chunks act as a contextual "memory" for the Gemini 2.0 Flash LLM, allowing it to generate answers specifically grounded in your PDF content.

Chat History: Maintains a clear conversational history, displaying both user questions and AI responses.

Internal Knowledge for Self-Explanation: The application can explain its own functionality, specifically how it processes and works with PDFs, without needing to query an external LLM. This addresses direct questions like "How do you work with PDFs?"

User-Friendly Interface: A simple and intuitive Streamlit web application.

üß† How It Works (Technical Flow)

The core of this application is the Retrieval-Augmented Generation (RAG) pattern, integrated with Streamlit for the user interface.

Document Upload & Text Extraction:

Users upload one or more .pdf files via the Streamlit file uploader (st.file_uploader).

The PyPDF2 library is used to read each PDF page and extract all available text content (get_pdf_text).

Text Chunking:

The extracted raw text (potentially very long) is then broken down into smaller, overlapping segments or "chunks" using LangChain's RecursiveCharacterTextSplitter (get_text_chunks). This is crucial because LLMs have token limits, and it helps focus the retrieval on more granular, relevant information.

Embedding & Vector Storage:

Each text chunk is transformed into a numerical vector (an "embedding") using GoogleGenerativeAIEmbeddings (models/embedding-001). These embeddings represent the semantic meaning of the text.

These embeddings, along with their corresponding text chunks, are then stored in a FAISS (Facebook AI Similarity Search) vector store (get_vector_store). FAISS is a library for efficient similarity search, allowing the system to quickly find text chunks that are semantically similar to a user's query.

Conversational Chain Setup (get_conversational_chain):

A ChatGoogleGenerativeAI model (gemini-2.0-flash) is initialized as the Large Language Model (LLM) for generating responses.

ConversationBufferMemory from LangChain is used to maintain the chat history, enabling multi-turn conversations where the LLM can remember previous interactions.

A ConversationalRetrievalChain is constructed. This chain takes a user's question, uses the FAISS vector store to retrieve relevant document chunks, and then feeds these chunks along with the chat history to the Gemini LLM to generate a coherent answer.

User Input Handling (handle_user_input):

When a user types a question:

Internal Self-Explanation: The system first checks if the question is about its own functionality (e.g., "How do you work with PDFs?"). If it is, a predefined, informative response is immediately provided, avoiding unnecessary LLM calls.

RAG Query: If the question is not an internal query, it's sent to the ConversationalRetrievalChain. This chain then performs the retrieval (finding relevant PDF chunks) and generation (using Gemini) steps.

The user's question and the bot's answer are appended to the st.session_state.chat_history and displayed in the Streamlit UI.

üöÄ Setup and Installation

To set up and run this application on your local machine, follow these steps:

Save the Code:

Save the provided Python code into a file named chat_with_pdfs.py (or app.py).

Create and Activate a Virtual Environment (Highly Recommended):
This isolates dependencies and prevents conflicts with other Python projects.

Open your terminal or command prompt.

Create a new virtual environment:

python -m venv pdf_chat_env

Activate the environment:

On Windows:

.\pdf_chat_env\Scripts\activate

On macOS/Linux:

source pdf_chat_env/bin/activate

Your prompt should change to (pdf_chat_env) indicating the environment is active.

Install Required Libraries:

With your pdf_chat_env activated, install all necessary Python packages:

pip install streamlit pypdf2 langchain langchain-google-genai langchain-community faiss-cpu

streamlit: For creating the web user interface.

pypdf2: To extract text from PDF files.

langchain: The core framework for building LLM applications.

langchain-google-genai: LangChain's integration with Google's Generative AI models (Gemini and Embeddings).

langchain-community: Contains various community-contributed LangChain integrations, including FAISS.

faiss-cpu: Facebook AI Similarity Search, a library for efficient similarity search (our vector store).

Obtain Google API Key:

You need a Google API Key with access to the Gemini API.

Go to Google AI Studio or Google Cloud Console.

Create a new API Key if you don't have one.

Set the Google API Key as an Environment Variable:
This is the most secure way to provide your API key to the application.

On Windows (Command Prompt, run BEFORE streamlit run):

set GOOGLE_API_KEY=YOUR_API_KEY_HERE

On Windows (PowerShell, run BEFORE streamlit run):

$env:GOOGLE_API_KEY="YOUR_API_KEY_HERE"

On macOS/Linux (Bash/Zsh, run BEFORE streamlit run):

export GOOGLE_API_KEY="YOUR_API_KEY_HERE"

Replace YOUR_API_KEY_HERE with your actual Google API Key.
Note: This key is set for the current terminal session. If you close and reopen the terminal, you'll need to set it again.

Run the Streamlit Application:

While your virtual environment is active and your API key environment variable is set, run the Streamlit command:

streamlit run chat_with_pdfs.py

This will open a new tab in your web browser with the "Chat with Your PDFs" application.

üéÆ Usage

Upload PDFs: On the left sidebar, click the "Browse files" button to select one or more PDF documents from your computer.

Process Documents: Click the "Process Documents" button. The application will extract text, chunk it, create embeddings, and set up the conversational chain. This may take a few moments depending on the size and number of documents.

Start Chatting: Once processing is complete, a chat input box will appear at the bottom of the main area. Type your questions related to the content of your uploaded PDFs and press Enter.

Example Interactions:

Ask about PDF content: "What is the main topic of this document?", "Can you summarize the introduction?", "What are the key points on page 3?"

Ask about the app's functionality: "How do you work with PDFs?", "Explain about the PDF processing.", "What do you do with my uploaded files?"

General questions (if processing is not done): "What is this app for?" (before PDFs are processed)

‚ö†Ô∏è Limitations

Local Processing Only: This version processes PDFs and creates the vector store in memory. It does not persist the vector store or chat history between sessions or across different users.

No Source Document Display: While the ConversationalRetrievalChain returns source documents, this UI currently only displays the answer, not the specific chunks from which the answer was derived.

PDF Text Extraction Quality: PyPDF2 relies on text layers in PDFs. Scanned PDFs without OCR or complex PDFs with unusual layouts may result in poor or no text extraction.

Single User Session: Designed for one user at a time. No multi-user support or real-time collaboration.

No File Storage: Uploaded PDFs are processed in memory and not stored on the server (unless you modify the code to do so).

API Key Management: For production, more robust API key management (e.g., Google Cloud Secret Manager) would be necessary.

üîÆ Future Enhancements

Persistent Vector Store: Implement saving and loading the FAISS vector store to disk or using a cloud-based vector database (e.g., Chroma, Pinecone, Google Cloud Vertex AI Vector Search) to avoid re-processing PDFs every time the app restarts.

Source Document Citation: Display the specific chunks or page numbers from the PDFs that the LLM used to formulate its answer.

Advanced UI Features: Add features like clearing chat history, showing processing progress bars more dynamically, or allowing users to select specific models.

Error Handling Improvements: More detailed user-facing error messages, and logging for debugging.

Multi-Modal PDF Analysis: Integrate image analysis (e.g., using CNNs) for PDFs that primarily contain scanned images or for detecting visual forgeries within documents.

Authentication and Multi-User Support: Add user authentication and mechanisms to store separate document libraries and chat histories for different users.

Web Deployment: Deploy the Streamlit application to a cloud platform (e.g., Streamlit Community Cloud, Google Cloud Run) for public access.

Custom Prompts/Templates: Allow users to define custom prompt templates for the LLM to guide its responses.